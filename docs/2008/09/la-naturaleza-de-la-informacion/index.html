<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow">
    <meta name="description" content="Un espacio para la experimentación y la reflexión">
    <meta name="author" content="Herbert Spencer">
    <meta name="copyright" content="(cc) Todos los contenidos bajo Creative Commons BY 4.0">
    <title>La Naturaleza de la Información · {dp} - doble página</title>

    <!-- Open Graph / Facebook -->
    <meta property="og:site_name" content="{dp} - doble página">
    <meta property="og:title" content="La Naturaleza de la Información">
    <meta property="og:description" content="Un espacio para la experimentación y la reflexión">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://herbertspencer.net/2008/09/la-naturaleza-de-la-informacion/">
    <meta property="og:image" content="https://herbertspencer.net/android-chrome-512x512.png">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="La Naturaleza de la Información">
    <meta name="twitter:description" content="Un espacio para la experimentación y la reflexión">
    <meta name="twitter:image" content="https://herbertspencer.net/android-chrome-512x512.png">
  
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/style.css">
  
    <!-- Feed -->
    <link rel="alternate" type="application/atom+xml" title="Feed de Posts" href="/feed.xml">
  
    <!-- Favicons -->
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  </head>
  
<body>
  <header>
    <div class="navbar">
        <h1 id="title"><a href="/">{dp} - doble página</a></h1>
        <!-- Botón toggle -->
        <a href="#" id="toggle-nav" aria-expanded="false">[+]</a>
    </div>

    <nav id="main" hidden>
      <ul>
        
        
          <li>
            <a class="nav-link" href="/about/herbert-spencer/">Acerca del autor</a>
          </li>
        
          <li>
            <a class="nav-link" href="/about/doble-pagina/">Doble Página</a>
          </li>
        
      </ul>
    </nav>
  
    <!-- Script de toggle -->
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        const btn = document.getElementById("toggle-nav");
        const nav = document.getElementById("main");
        btn.addEventListener("click", function (e) {
          e.preventDefault();
          const isHidden = nav.hasAttribute("hidden");
          nav.toggleAttribute("hidden");
          btn.textContent = isHidden ? "[–]" : "[+]";
          btn.setAttribute("aria-expanded", isHidden);
        });
      });
    </script>
 </header>


  <main id="dp">
    <sidebar>
      <h1 class="title">La Naturaleza de la Información</h1>

      <!-- Imagen (si image.path existe y no tiene sidebar: false) -->
      
        <figure>
          <img
            src="/assets/uploads/2008/09/info-sketch1.png"
            alt=""
            class=""
            style=""
          />
        </figure>
      

      <!-- P5 (si p5.script existe y no tiene sidebar: false) -->
      

      <!-- Vimeo (si vimeo.url existe y no tiene sidebar: false) -->
      

      <!-- YouTube (si youtube.url existe y no tiene sidebar: false) -->
      

      <!-- Fecha -->
      <time datetime="2008-09-11T01:40:59+12:00">
        11 September 2008
      </time>

      <!-- Bloque de sidebar en HTML directo -->
      

      <!-- Bloque de sidebar en MD (convertido a HTML) -->
      

      <a id="btn-citar-la-naturaleza-de-la-informacion" class="btn-citar">&#x301E;</a>
  
  <div id="modal-citar-la-naturaleza-de-la-informacion" class="modal-citar">
    <div class="modal-contenido">
      <span class="modal-cerrar">&times;</span>
      <h3>Referencias</h3>
  
      <h4>APA (7ª edición)</h4>
      <p id="apa-cita">
        Spencer, H. (2008, September 11). <em>La Naturaleza de la Información</em>. 
        En <em>{dp} - doble página</em>. https://herbertspencer.net/2008/09/la-naturaleza-de-la-informacion/. Visitado en: <span class="fecha-acceso"></span>
      </p>
  
      <h4>MLA</h4>
      <p id="mla-cita">
        Spencer, Herbert. "<em>La Naturaleza de la Información</em>." <em>{dp} - doble página</em>, 11 September 2008, 
        https://herbertspencer.net/2008/09/la-naturaleza-de-la-informacion/. Accedido el <span class="fecha-acceso"></span>.
      </p>
  
      <h4>Chicago</h4>
      <p id="chicago-cita">
        Spencer, Herbert. "<em>La Naturaleza de la Información</em>." {dp} - doble página. Publicado el 11 de September de 2008. 
        https://herbertspencer.net/2008/09/la-naturaleza-de-la-informacion/. Consultado el <span class="fecha-acceso"></span>.
      </p>
    </div>
  </div>
  
  <script>
  document.addEventListener("DOMContentLoaded", function () {
    const btn = document.getElementById("btn-citar-la-naturaleza-de-la-informacion");
    const modal = document.getElementById("modal-citar-la-naturaleza-de-la-informacion");
    const cerrar = modal.querySelector(".modal-cerrar");
  
    btn.addEventListener("click", () => modal.style.display = "block");
    cerrar.addEventListener("click", () => modal.style.display = "none");
    window.addEventListener("click", e => {
      if (e.target == modal) modal.style.display = "none";
    });
  
    // Insertar fecha de acceso en formato: 26 de marzo de 2025
    const fecha = new Date();
    const meses = [
      "enero", "febrero", "marzo", "abril", "mayo", "junio",
      "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre"
    ];
    const fechaFormateada = `${fecha.getDate()} de ${meses[fecha.getMonth()]} de ${fecha.getFullYear()}`;
    document.querySelectorAll(".fecha-acceso").forEach(el => el.textContent = fechaFormateada);
  });
  </script>
    </sidebar>

    <article>
      <p>El concepto de información es bastante abstracto y elusivo pero lo usamos con tanta familiaridad y tan cotidianamente que ni siquiera nos tomamos la molestia en definirla. Al menos sabemos 2 cosas básicas (o creemos 2 mitos básicos): que se constituye como el <strong>contenido</strong> del acto comunicativo y que es algo <strong>inmaterial</strong> (tal vez por la facilidad con que transita desde la boca al oído). La información emerge como contenido, como una especie de “cosa” u “objeto” pero con cualidades especiales: facilidad de reproducción, amplificación y propagación; pero a la vez intangible y volátil. Si tuviésemos que ponerle fecha de inicio, la Era de la Información comenzó el año 1948, poco tiempo después de finalizar la Segunda Guerra Mundial cuando Claude Shannon publicó el trabajo “<strong>Una Teoría Matemática de la Comunicación</strong> “ <sup id="fnref:1"><a href="#fn:1" class="footnote-ref">1</a></sup> . <a href="/assets/uploads/2008/09/esquema-informacion1.png"><img src="/assets/uploads/2008/09/esquema-informacion1.png" alt="Diagrama esquemático general de los sistemas de comunicación (copiado sin permiso) de Claude Shannon, 1948." /></a> En este trabajo, Shannon define la información a partir de la relación entre <strong>señal</strong> y <strong>ruido</strong> ; <em>la información es una medida estadística de la incertidumbre del sistema</em>(dentro de un sistema entrópico). La información implica una relación <strong>no-lineal</strong> y <strong>no-determinista</strong> entre los niveles micro y macroscópicos del sistema físico.</p>

<blockquote>
  <p>El problema fundamental de la comunicación consiste en reproducir en un punto exactamente (o con cierto grado de aproximación) un mensage seleccionado desde otro punto <sup id="fnref:2"><a href="#fn:2" class="footnote-ref">2</a></sup></p>
</blockquote>

<p>Según el testimonio de investigadores (como Claude Shannon y Warren Weaver) trabajando para las compañias de telecomunicaciones <sup id="fnref:3"><a href="#fn:3" class="footnote-ref">3</a></sup> durante la primera mitad del siglo XX, la teoría de la información se ocupó fundamentalmente de la problemática de la reproducción de señales codificadas a través de un canal de manera rápida, económica y eficiente. Cuando una señal que viaja por un canal frecuentemente produce una estática de fondo. Esta estática no se resuelve simplemente amplificando la señal ya que esto sólo “inunda la señal en su propia energía” y la vuelve indistinguible del ruido. El problema se resolvió parcialmente implementando distintos filtros, lo que volvió necesario desarrollar formas para codificar la señal de manera tal que minimizara los posibles errores producto del ruído y la distorsión. Entonces se desarrollaron códigos con <strong>patrones redundantes</strong> que sirvieran como verificación y corrección de la señal. Esto implicó una aproximación matemática-estadística al problema, un método que entendiera la información como una entidad separada de toda significación.</p>

<blockquote>
  <p>…“frecuentemente las señales tienen significado; esto es, se refieren o tienen correlación con un sistema compuesto por ciertas entidades físicas o conceptuales. Estos aspectos semánticos de la comunicación son irrelevantes para el problema de la ingeniería” <sup id="fnref:4"><a href="#fn:4" class="footnote-ref">4</a></sup></p>
</blockquote>

<p>Es curioso ver que, visto desde este paradigma científico, un mensaje cargado de significación podría equivaler en términos estadísticos a otro completamente vacío e incoherente. Este pensamiento viene de una <strong>concepción biunívoca</strong> de la información; basta ver el esquema más arriba. No considera <strong>el modo multiunívoco del medio digital</strong> , mucho menos los <strong>efectos mediáticos</strong> del mensaje que ahora vive en un espacio de señales y protocolos abstractos, porque el contexto se constituye como un lugar. Estos efectos colaterales no podrían caer en la categoría de “ruido” definida por el modelo Shannon-Weaver. Otro aspecto significativo es el tipo de matemática que está involucrada para describir los fenómenos de la información. Se trata del modelo matemático de la <strong>termodinámica</strong> y la <strong>mecánica estadística</strong> <sup id="fnref:5"><a href="#fn:5" class="footnote-ref">5</a></sup>. La entropía de un sistema, entonces, estará medida respecto al pordentaje de nuestra incertidumbre respecto de un determinado sistema de codificación o lenguaje <sup id="fnref:6"><a href="#fn:6" class="footnote-ref">6</a></sup>. Entonces, a pesar de Shannon declaró que su teoría matemática de la información no guarda relación —o trabaja en un distinto nivel— respecto de la capa semántica, establece una relación directa entre la información y la entropía. Aplicaciones de esta mirada estadística las podemos observar en las cadenas de Markòv. Este mecanismo permite crear textos <sup id="fnref:7"><a href="#fn:7" class="footnote-ref">7</a></sup> a partir de otros textos que funcionan como corpus referencial (<a href="http://www.eskimo.com/~rstarr/poormfa/markov.html" title="Generador de textos mediante cadenas de Markòv \(inglés\)">Acá</a> un ejemplo para hacer poemas). Google <a href="http://es.wikipedia.org/wiki/PageRank" title="definición en Wikipedia">PageRank</a> <sup id="fnref:8"><a href="#fn:8" class="footnote-ref">8</a></sup> no es otra cosa que un conjunto inmenso de cadenas de Markòv en un grafo dirigido que está compuesto por toda la red. Cuando hablamos de SEO <sup id="fnref:9"><a href="#fn:9" class="footnote-ref">9</a></sup>, hablamos de escribir con el mayor grado de redundancia posible para que nos ubiquemos al centro del <strong>cluster semántico</strong> al cual queremos adscribir nuestro contenido. La pregunta es: dentro de un esquema estadístico de la visibilidad y la relevancia, ¿qué lugar tienen las particularidades (altamente entrópicas) ? ¿los puntos de vista excéntricos? ¿los folklorismos? ¿las cosas únicas y especiales? <img src="/assets/uploads/2006/07/stars.png" alt="" /> Ocurre que para medir el grado de entropía hay que recurrir a un <strong>corpus de referencia</strong> , ya que se trata de un valor puramente estadístico. Es decir, algo es frecuente o infrecuente, probable o improbable, de acuerdo a una cantidad enorme de casos catastrados como referencia. He ahí la falacia de la teoría: considerar un corpus lo suficientemente grande como para suponer un absoluto, un eje del sistema, es una fantasía. Primero, porque ni el lenguaje ni los medios, ni las conversaciones son estáticas; viven en su lenguajear. Segundo porque el corpus siempre será inexacto y tendrá muchos puntos ciegos. Desde ahí, la pregunta por la relevancia se vuelve mucho más compleja. De hecho, no vive en los extremos de la redundancia o la entropía.</p>

<div class="footnotes">
  <hr />

  <ol>
<li id="fn:1">C.E. Shannon, “<a href="http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf" title="http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">A Mathematical Theory of Communication</a>”, <em><a href="http://www.alcatel-lucent.com/wps/portal/!ut/p/kcxml/04_Sj9SPykssy0xPLMnMz0vM0Y_QjzKLd4w3MTfVL8h2VAQAYbZ-ZQ!!?LMSG_CABINET=Bell_Labs&amp;LMSG_CONTENT_FILE=Resources/Bell_Labs_Technical_Journal.xml" title="Bell System Technical Journal">Bell System Technical Journal</a></em> , vol. 27, pp. 379-423, 623-656, July, October, 1948. <a href="#fnref:1" class="footnote-back">↩</a></li>
<li id="fn:2">Ibid. pág. 1. <a href="#fnref:2" class="footnote-back">↩</a></li>
<li id="fn:3">en este caso, <em>Bell Industries</em> <a href="#fnref:3" class="footnote-back">↩</a></li>
<li id="fn:4">Ibid. pág. 5. <a href="#fnref:4" class="footnote-back">↩</a></li>
<li id="fn:5">El concepto de entropía tratado por Shannon es heredero directo del modelo Maxwell-Boltzmann para la descripción de los fenómenos termodinámicos que, por definición, no pueden hacerse cargo de precedir la ubicación de cada partícula sino que se describe las probabilidades del estado de determinado gas a un nivel macro. <a href="#fnref:5" class="footnote-back">↩</a></li>
<li id="fn:6">La <strong>redundancia</strong> dentro de un lenguaje se mide como el índice de coincidencia. El IC en el idioma inglés es de 0.667 según William Friendman, “The Index of Coincidence and its Applications in Cryptography.” <a href="#fnref:6" class="footnote-back">↩</a></li>
<li id="fn:7">Texto entendido en el amplio sentido, ya que esta técnica se ha empleado para componer melodías dentro de un estilo, o crear publicaciones científicas completamente autómatas, por citar casos anecdóticos <a href="#fnref:7" class="footnote-back">↩</a></li>
<li id="fn:8">El algoritmo mediante el cual Google construye el ranking que entrega en los resultados de bísqueda. <a href="#fnref:8" class="footnote-back">↩</a></li>
<li id="fn:9">Search Engine Optimization: técnica para escribir los textos y disponer el código XHTML de una página para optimizar su visibilidad, por lo tanto rankeo, dentro de la lógica de los buscadores <a href="#fnref:9" class="footnote-back">↩</a></li>
</ol>
</div>

      <script src="https://utteranc.es/client.js"
        repo="hspencer/hspencer.github.io"
        issue-term="title"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
    </article>
  </main>

  <footer>
   <p>
     <a href="/about/doble-pagina">{doble página}</a> es el sitio personal de 
     <a href="/about/herbert-spencer">Herbert Spencer</a>, iniciado en 2003.  
     Los contenidos están bajo licencia 
     <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="license noopener">CC BY-SA 4.0</a>.  
   </p>
   <p>
     El sitio está generado con <a href="https://jekyllrb.com/" target="_blank" rel="noopener">Jekyll</a>, 
     el código fuente está en <a href="https://github.com/hspencer/dp" target="_blank" rel="noopener">GitHub</a>, 
     y los comentarios están habilitados vía <a href="https://utteranc.es/" target="_blank" rel="noopener">utterances</a>.
   </p>
   <p>
     Última actualización: 2 April 2025 - <a href="/feed.xml">Feed RSS</a>.
   </p>
 </footer>
</body>
</html>